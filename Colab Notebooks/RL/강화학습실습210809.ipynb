{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"강화학습실습210809.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOH2KM1SEkO7jlXcNrij4Kg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ABmqMqULE-_C"},"source":["# 210809 강화학습 실습\n","\n","강화학습 기초: 그리드 월드와 다이내믹 프로그래밍 실습\n","--\n","\n","출처 :파이썬과 케라스로 배우는 강화학습\n","\n","- 이웅원, 양혁렬, 김건우, 이영무, 이의령 지음"]},{"cell_type":"code","metadata":{"id":"np1Ntd3PFR7i"},"source":["!pip install environment"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v435gqx2HrWA"},"source":["environment.py\n","--\n","\n","출처: https://github.com/rlcode/reinforcement-learning-kr-v2/blob/master/1-grid-world/1-policy-iteration/environment.py"]},{"cell_type":"code","metadata":{"id":"1b4DVgbnFtnJ","executionInfo":{"status":"ok","timestamp":1628498752547,"user_tz":-540,"elapsed":795,"user":{"displayName":"YS YS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOUuu8fLL2hz4JnGsBT2pxIvnZHyZkbgfU2U1zaVY=s64","userId":"10253263109621147001"}}},"source":["import environment"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSXMDAWuFjcB","executionInfo":{"status":"ok","timestamp":1628499317062,"user_tz":-540,"elapsed":639,"user":{"displayName":"YS YS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOUuu8fLL2hz4JnGsBT2pxIvnZHyZkbgfU2U1zaVY=s64","userId":"10253263109621147001"}}},"source":["import tkinter as tk\n","from tkinter import Button\n","import time\n","import numpy as np\n","from PIL import ImageTk, Image\n","\n","PhotoImage = ImageTk.PhotoImage\n","UNIT = 100  # 픽셀 수\n","HEIGHT = 5  # 그리드월드 세로\n","WIDTH = 5  # 그리드월드 가로\n","TRANSITION_PROB = 1\n","POSSIBLE_ACTIONS = [0, 1, 2, 3]  # 좌, 우, 상, 하\n","ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 좌표로 나타낸 행동\n","REWARDS = []\n","\n","\n","class GraphicDisplay(tk.Tk):\n","    def __init__(self, agent):\n","        super(GraphicDisplay, self).__init__()\n","        self.title('Policy Iteration')\n","        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT + 50))\n","        self.texts = []\n","        self.arrows = []\n","        self.env = Env()\n","        self.agent = agent\n","        self.evaluation_count = 0\n","        self.improvement_count = 0\n","        self.is_moving = 0\n","        (self.up, self.down, self.left, self.right), self.shapes = self.load_images()\n","        self.canvas = self._build_canvas()\n","        self.text_reward(2, 2, \"R : 1.0\")\n","        self.text_reward(1, 2, \"R : -1.0\")\n","        self.text_reward(2, 1, \"R : -1.0\")\n","\n","    def _build_canvas(self):\n","        canvas = tk.Canvas(self, bg='white',\n","                           height=HEIGHT * UNIT,\n","                           width=WIDTH * UNIT)\n","        # 버튼 초기화\n","        iteration_button = Button(self, text=\"Evaluate\",\n","                                  command=self.evaluate_policy)\n","        iteration_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.13, HEIGHT * UNIT + 10,\n","                             window=iteration_button)\n","        policy_button = Button(self, text=\"Improve\",\n","                               command=self.improve_policy)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.37, HEIGHT * UNIT + 10,\n","                             window=policy_button)\n","        policy_button = Button(self, text=\"move\", command=self.move_by_policy)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.62, HEIGHT * UNIT + 10,\n","                             window=policy_button)\n","        policy_button = Button(self, text=\"reset\", command=self.reset)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.87, HEIGHT * UNIT + 10,\n","                             window=policy_button)\n","\n","        # 그리드 생성\n","        for col in range(0, WIDTH * UNIT, UNIT):  # 0~400 by 80\n","            x0, y0, x1, y1 = col, 0, col, HEIGHT * UNIT\n","            canvas.create_line(x0, y0, x1, y1)\n","        for row in range(0, HEIGHT * UNIT, UNIT):  # 0~400 by 80\n","            x0, y0, x1, y1 = 0, row, HEIGHT * UNIT, row\n","            canvas.create_line(x0, y0, x1, y1)\n","\n","        # 캔버스에 이미지 추가\n","        self.rectangle = canvas.create_image(50, 50, image=self.shapes[0])\n","        canvas.create_image(250, 150, image=self.shapes[1])\n","        canvas.create_image(150, 250, image=self.shapes[1])\n","        canvas.create_image(250, 250, image=self.shapes[2])\n","\n","        canvas.pack()\n","\n","        return canvas\n","\n","    def load_images(self):\n","        up = PhotoImage(Image.open(\"../img/up.png\").resize((13, 13)))\n","        right = PhotoImage(Image.open(\"../img/right.png\").resize((13, 13)))\n","        left = PhotoImage(Image.open(\"../img/left.png\").resize((13, 13)))\n","        down = PhotoImage(Image.open(\"../img/down.png\").resize((13, 13)))\n","        rectangle = PhotoImage(Image.open(\"../img/rectangle.png\").resize((65, 65)))\n","        triangle = PhotoImage(Image.open(\"../img/triangle.png\").resize((65, 65)))\n","        circle = PhotoImage(Image.open(\"../img/circle.png\").resize((65, 65)))\n","        return (up, down, left, right), (rectangle, triangle, circle)\n","\n","    def reset(self):\n","        if self.is_moving == 0:\n","            self.evaluation_count = 0\n","            self.improvement_count = 0\n","            for i in self.texts:\n","                self.canvas.delete(i)\n","\n","            for i in self.arrows:\n","                self.canvas.delete(i)\n","            self.agent.value_table = [[0.0] * WIDTH for _ in range(HEIGHT)]\n","            self.agent.policy_table = ([[[0.25, 0.25, 0.25, 0.25]] * WIDTH\n","                                        for _ in range(HEIGHT)])\n","            self.agent.policy_table[2][2] = []\n","            x, y = self.canvas.coords(self.rectangle)\n","            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n","\n","    def text_value(self, row, col, contents, font='Helvetica', size=10,\n","                   style='normal', anchor=\"nw\"):\n","        origin_x, origin_y = 85, 70\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def text_reward(self, row, col, contents, font='Helvetica', size=10,\n","                    style='normal', anchor=\"nw\"):\n","        origin_x, origin_y = 5, 5\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def rectangle_move(self, action):\n","        base_action = np.array([0, 0])\n","        location = self.find_rectangle()\n","        self.render()\n","        if action == 0 and location[0] > 0:  # 상\n","            base_action[1] -= UNIT\n","        elif action == 1 and location[0] < HEIGHT - 1:  # 하\n","            base_action[1] += UNIT\n","        elif action == 2 and location[1] > 0:  # 좌\n","            base_action[0] -= UNIT\n","        elif action == 3 and location[1] < WIDTH - 1:  # 우\n","            base_action[0] += UNIT\n","        # move agent\n","        self.canvas.move(self.rectangle, base_action[0], base_action[1])\n","\n","    def find_rectangle(self):\n","        temp = self.canvas.coords(self.rectangle)\n","        x = (temp[0] / 100) - 0.5\n","        y = (temp[1] / 100) - 0.5\n","        return int(y), int(x)\n","\n","    def move_by_policy(self):\n","        if self.improvement_count != 0 and self.is_moving != 1:\n","            self.is_moving = 1\n","\n","            x, y = self.canvas.coords(self.rectangle)\n","            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n","\n","            x, y = self.find_rectangle()\n","            while len(self.agent.policy_table[x][y]) != 0:\n","                self.after(100,\n","                           self.rectangle_move(self.agent.get_action([x, y])))\n","                x, y = self.find_rectangle()\n","            self.is_moving = 0\n","\n","    def draw_one_arrow(self, col, row, policy):\n","        if col == 2 and row == 2:\n","            return\n","\n","        if policy[0] > 0:  # up\n","            origin_x, origin_y = 50 + (UNIT * row), 10 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.up))\n","        if policy[1] > 0:  # down\n","            origin_x, origin_y = 50 + (UNIT * row), 90 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.down))\n","        if policy[2] > 0:  # left\n","            origin_x, origin_y = 10 + (UNIT * row), 50 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.left))\n","        if policy[3] > 0:  # right\n","            origin_x, origin_y = 90 + (UNIT * row), 50 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.right))\n","\n","    def draw_from_policy(self, policy_table):\n","        for i in range(HEIGHT):\n","            for j in range(WIDTH):\n","                self.draw_one_arrow(i, j, policy_table[i][j])\n","\n","    def print_value_table(self, value_table):\n","        for i in range(WIDTH):\n","            for j in range(HEIGHT):\n","                self.text_value(i, j, round(value_table[i][j], 2))\n","\n","    def render(self):\n","        time.sleep(0.1)\n","        self.canvas.tag_raise(self.rectangle)\n","        self.update()\n","\n","    def evaluate_policy(self):\n","        self.evaluation_count += 1\n","        for i in self.texts:\n","            self.canvas.delete(i)\n","        self.agent.policy_evaluation()\n","        self.print_value_table(self.agent.value_table)\n","\n","    def improve_policy(self):\n","        self.improvement_count += 1\n","        for i in self.arrows:\n","            self.canvas.delete(i)\n","        self.agent.policy_improvement()\n","        self.draw_from_policy(self.agent.policy_table)\n","\n","\n","class Env:\n","    def __init__(self):\n","        self.transition_probability = TRANSITION_PROB\n","        self.width = WIDTH\n","        self.height = HEIGHT\n","        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n","        self.possible_actions = POSSIBLE_ACTIONS\n","        self.reward[2][2] = 1  # (2,2) 좌표 동그라미 위치에 보상 1\n","        self.reward[1][2] = -1  # (1,2) 좌표 세모 위치에 보상 -1\n","        self.reward[2][1] = -1  # (2,1) 좌표 세모 위치에 보상 -1\n","        self.all_state = []\n","\n","        for x in range(WIDTH):\n","            for y in range(HEIGHT):\n","                state = [x, y]\n","                self.all_state.append(state)\n","\n","    def get_reward(self, state, action):\n","        next_state = self.state_after_action(state, action)\n","        return self.reward[next_state[0]][next_state[1]]\n","\n","    def state_after_action(self, state, action_index):\n","        action = ACTIONS[action_index]\n","        return self.check_boundary([state[0] + action[0], state[1] + action[1]])\n","\n","    @staticmethod\n","    def check_boundary(state):\n","        state[0] = (0 if state[0] < 0 else WIDTH - 1\n","                    if state[0] > WIDTH - 1 else state[0])\n","        state[1] = (0 if state[1] < 0 else HEIGHT - 1\n","                    if state[1] > HEIGHT - 1 else state[1])\n","        return state\n","\n","    def get_transition_prob(self, state, action):\n","        return self.transition_probability\n","\n","    def get_all_states(self):\n","        return self.all_state"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N32Dq0EZH8LZ"},"source":["policy_iteration.py\n","--\n","\n","출처: https://github.com/rlcode/reinforcement-learning-kr-v2/blob/master/1-grid-world/1-policy-iteration/policy_iteration.py\n"]},{"cell_type":"code","metadata":{"id":"JxpEdzAfH4pK","executionInfo":{"status":"ok","timestamp":1628501381816,"user_tz":-540,"elapsed":453,"user":{"displayName":"YS YS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOUuu8fLL2hz4JnGsBT2pxIvnZHyZkbgfU2U1zaVY=s64","userId":"10253263109621147001"}}},"source":["import numpy as np\n","\n","class PolicyIteration:\n","    def __init__(self, env):\n","        # 환경에 대한 객체 선언\n","        self.env=env\n","        # 가치함수를 2차원 리스트로 초기화\n","        self.value_table=[[0,0]*env.width for _ in range(env.height)]\n","        # 상하좌우 동일한 확률로 정책 초기화\n","        self.policy_table=[[[0.25,0.25,0.25,0.25]]*env.width for _ in range(env.height)]\n","        # 마침 상태의 설정\n","        self.policy_table[2][2]=[]\n","        # 할인율\n","        self.discount_factor=0.9\n","\n","        # 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가\n","    def policy_evaluation(self):\n","        # 다음 가치함수 초기화\n","        next_value_table=[[0.00]*self.env.width for _ in range(self.env.height)]\n","\n","        # 모든 상태에 대해서 벨만 기대 방정식을 계산\n","        for state in self.env.get_all_states():\n","            value=0.0\n","            # 마침 상태의 가치핰수 = 0\n","            if state == [2,2]:\n","                next_value_table[state[0]][state[1]]=value\n","                continue\n","\n","            # 벨만 기대 방정식\n","            for action in self.env.possible_actions:\n","                next_state=self.env.state_after_action(state, action)\n","                reward=self.env.get_reward(state, action)\n","                next_value=self.get_value(next_state)\n","                value += (self.get_policy(state)[action] *\n","                          (reward+self.discount_factor*next_value))\n","                \n","            next_value_table[state[0]][state[1]]=value\n","        self.value_table=next_value_table\n","\n","        # 현재 가치함수에 대해서 탐욕 정책 발전\n","    def policy_improvement(self):\n","        next_policy=self.policy_table\n","        for state in self.env.get_all_states():\n","            if state==[2,2]:\n","                continue\n","\n","            value_list=[]\n","            # 반환할 정책 초기화\n","            result=[0.0,0.0,0.0,0.0]\n","\n","        #모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n","        for index, action in enumerate(self.env.possible_actions):\n","            next_state=self.env.state_after_action(state, action)\n","            reward=self.env.get_reward(state, action)\n","            next_value=self.get_value(next_state)\n","            value=reward+self.discount_factor*next_value\n","            value_list.append(value)\n","\n","        # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n","        max_idx_list=np.argwhere(value_list=np.amax(value_list))\n","        max_idx_list=max_idx_list.flatten().tolist()\n","        prob=1/len(max_idx_list)\n","\n","        for idx in max_idx_list:\n","            result[idx]=prob\n","\n","        next_policy[state[0]][state[1]]=result\n","\n","        self.policy_table=next_policy\n","\n","    # 특정 상태에서 정책에 따라 무작위로 행동을 반환\n","    def get_action(self, state):\n","        policy=self.get_policy(state)\n","        policy=np.array(policy)\n","        return np.random.choice(4,1,p=policy)[0]\n","\n","    # 상태에 따른 정책 반환\n","    def get_policy(self, state):\n","        return self.policy_table[state[0]][state[1]]\n","\n","    #  가치함수의 값을 반환\n","    def get_value(self, state):\n","        return self.value_table[state[0]][state[1]]"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NgaGcFFKMFk","executionInfo":{"status":"ok","timestamp":1628501394612,"user_tz":-540,"elapsed":428,"user":{"displayName":"YS YS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOUuu8fLL2hz4JnGsBT2pxIvnZHyZkbgfU2U1zaVY=s64","userId":"10253263109621147001"}}},"source":["#if __name__==\"__main__\":\n","#    env=Env()\n","#    policy_iteration=PolicyIteration(env)\n","#    grid_world=GraphicDisplay(policy_iteration)\n","#    grid_world.mainloop()"],"execution_count":22,"outputs":[]}]}